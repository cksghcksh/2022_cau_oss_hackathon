{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1AosAX9DXOlc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cksghcksh/2022_cau_oss_hackathon/blob/main/hackathon_team04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        "*   모든 구현은 [2. 데이터 전처리] 및 [3.모델 생성]에서만 진행\n",
        "*   [4. 모델 저장]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *    트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임 -> 런타임 연결 해제 및 삭제\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        " *    \"런타임 -> 런타임 다시시작\"은 클라우드 스토리지에 저장된 모델은 유지됨\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        " *  모델 발표 자료 \n",
        "* 제출 기한: **오후 6시 (단, 발표자료는 12시)**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2022_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드가 적발될 경우\n",
        "*  Pre-trained 모델을 사용한 경우 (transfer learning 포함)\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4525d49c-aec2-4850-e453-5befe478efb5"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from keras import datasets, layers, models\n",
        "\n",
        "# 데이터셋 로드 (MNIST, fashion-MNIST, Kujushiji-MNIST, MNIST_corrupted (test only))\n",
        "train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "\n",
        "train_ds2, test_ds2 = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds2['label'] += 10;\n",
        "test_ds2['label'] += 10;\n",
        "\n",
        "train_ds3, test_ds3 = tfds.load('kmnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds3['label'] += 20;\n",
        "test_ds3['label'] += 20;\n",
        "\n",
        "test_ds4 = tfds.load('mnist_corrupted/zigzag', split='test', shuffle_files=False, batch_size=-1)\n",
        "\n",
        "# 데이터셋 병합 (training: 180,000개, test: 40,000개)\n",
        "x_train = np.append(np.append(train_ds['image'], train_ds2['image'], 0), train_ds3['image'], 0);\n",
        "y_train = np.append(np.append(train_ds['label'], train_ds2['label'], 0), train_ds3['label'], 0);\n",
        "\n",
        "x_test = np.append(np.append(np.append(test_ds['image'], test_ds2['image'], 0), test_ds3['image'], 0), test_ds4['image'], 0);\n",
        "y_test = np.append(np.append(np.append(test_ds['label'], test_ds2['label'], 0), test_ds3['label'], 0), test_ds4['label'], 0);\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수: 30, 입력 데이터 구조: (28, 28, 1)\n",
        "num_classes = y_train.shape[1]\n",
        "input_shape = x_train.shape[1:]\n",
        "print(num_classes, input_shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 (28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9"
      },
      "source": [
        "# **2. 데이터 전처리**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9KWTBP6AI1"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "# 원본 데이터와 전처리 후 데이터를 구분하기 위해, 변수명 x_train_after, x_test_after를 변경하지 말 것\n",
        "x_train_after = x_train / 255\n",
        "x_test_after = x_test / 255"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 랜덤시드 고정시키기\n",
        "np.random.seed(3)\n",
        "\n",
        "#데이터 증식 설정하기\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomZoom(0.2),          \n",
        "        layers.RandomCrop(200, 200, seed=42),\n",
        "        layers.RandomWidth((0.2, 0.3))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "6QlcSd2z-Z-E"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-lo-O1yiFpY"
      },
      "source": [
        "# **3. 모델 생성**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## residual block을 포함한 CNN"
      ],
      "metadata": {
        "id": "cYdqJmRyJH7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = data_augmentation(inputs) #랜덤하게 증식된 이미지 데이터 입력\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "#x = BatchNormalization()(x)\n",
        "#x = layers.Activation(\"leakyrelu\")\n",
        "residual = x\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2, padding=\"same\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "residual = layers.Conv2D(32, 1, strides=2)(residual)\n",
        "x = layers.add([x, residual])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "residual = x\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2, padding=\"same\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)\n",
        "x = layers.add([x, residual])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "outputs = layers.Dense(30, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWm8fHmGiYiO",
        "outputId": "24f5d4b0-9b51-4d75-8b01-5b220ed22fe1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 28, 28, 1)    0           ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)             (None, 26, 26, 32)   320         ['sequential[10][0]']            \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)            (None, 26, 26, 32)   9248        ['conv2d_99[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)            (None, 26, 26, 32)   9248        ['conv2d_100[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_21 (MaxPooling2D  (None, 13, 13, 32)  0           ['conv2d_101[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 13, 13, 32)   0           ['max_pooling2d_21[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)            (None, 13, 13, 32)   1056        ['conv2d_99[0][0]']              \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 13, 13, 32)   0           ['dropout_20[0][0]',             \n",
            "                                                                  'conv2d_102[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 13, 13, 32)   0           ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)            (None, 11, 11, 64)   18496       ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)            (None, 11, 11, 64)   36928       ['conv2d_103[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)            (None, 11, 11, 64)   36928       ['conv2d_104[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_22 (MaxPooling2D  (None, 6, 6, 64)    0           ['conv2d_105[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 6, 6, 64)     0           ['max_pooling2d_22[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)            (None, 6, 6, 64)     4160        ['conv2d_103[0][0]']             \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 6, 6, 64)     0           ['dropout_22[0][0]',             \n",
            "                                                                  'conv2d_106[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 6, 6, 64)     0           ['add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_107 (Conv2D)            (None, 4, 4, 128)    73856       ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)            (None, 2, 2, 128)    147584      ['conv2d_107[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_23 (MaxPooling2D  (None, 1, 1, 128)   0           ['conv2d_108[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3 (Gl  (None, 128)         0           ['max_pooling2d_23[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 30)           3870        ['global_average_pooling2d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 341,694\n",
            "Trainable params: 341,694\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, mode = 'auto')\n",
        "\n",
        "#모델 훈련\n",
        "model.fit(x_train_after, y_train, batch_size = 64, epochs = 100, shuffle=True, callbacks=[cp_callback, early_stopping], validation_data=(x_test_after, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvutNnjXibab",
        "outputId": "3c8fb3bf-e586-45aa-c757-c679790373ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.8844 - accuracy: 0.7131\n",
            "Epoch 1: val_accuracy improved from -inf to 0.73410, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 33s 11ms/step - loss: 0.8843 - accuracy: 0.7132 - val_loss: 1.0925 - val_accuracy: 0.7341\n",
            "Epoch 2/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.4900 - accuracy: 0.8340\n",
            "Epoch 2: val_accuracy improved from 0.73410 to 0.78882, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 32s 11ms/step - loss: 0.4899 - accuracy: 0.8340 - val_loss: 0.8035 - val_accuracy: 0.7888\n",
            "Epoch 3/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.8597\n",
            "Epoch 3: val_accuracy improved from 0.78882 to 0.79137, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.4123 - accuracy: 0.8597 - val_loss: 0.8657 - val_accuracy: 0.7914\n",
            "Epoch 4/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8705\n",
            "Epoch 4: val_accuracy improved from 0.79137 to 0.81767, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.3770 - accuracy: 0.8705 - val_loss: 0.6848 - val_accuracy: 0.8177\n",
            "Epoch 5/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8793\n",
            "Epoch 5: val_accuracy improved from 0.81767 to 0.83368, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.3502 - accuracy: 0.8793 - val_loss: 0.7003 - val_accuracy: 0.8337\n",
            "Epoch 6/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.8850\n",
            "Epoch 6: val_accuracy improved from 0.83368 to 0.83947, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 32s 11ms/step - loss: 0.3361 - accuracy: 0.8850 - val_loss: 0.5880 - val_accuracy: 0.8395\n",
            "Epoch 7/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8893\n",
            "Epoch 7: val_accuracy did not improve from 0.83947\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.3232 - accuracy: 0.8893 - val_loss: 0.7167 - val_accuracy: 0.8266\n",
            "Epoch 8/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8929\n",
            "Epoch 8: val_accuracy improved from 0.83947 to 0.84447, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.3126 - accuracy: 0.8929 - val_loss: 0.6270 - val_accuracy: 0.8445\n",
            "Epoch 9/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8942\n",
            "Epoch 9: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 32s 11ms/step - loss: 0.3082 - accuracy: 0.8942 - val_loss: 0.6411 - val_accuracy: 0.8352\n",
            "Epoch 10/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8971\n",
            "Epoch 10: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 33s 12ms/step - loss: 0.2986 - accuracy: 0.8971 - val_loss: 0.6719 - val_accuracy: 0.8423\n",
            "Epoch 11/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.8988\n",
            "Epoch 11: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 33s 12ms/step - loss: 0.2955 - accuracy: 0.8988 - val_loss: 0.7580 - val_accuracy: 0.8339\n",
            "Epoch 12/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8993\n",
            "Epoch 12: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2922 - accuracy: 0.8993 - val_loss: 0.7899 - val_accuracy: 0.8337\n",
            "Epoch 13/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9012\n",
            "Epoch 13: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2876 - accuracy: 0.9012 - val_loss: 0.6640 - val_accuracy: 0.8431\n",
            "Epoch 14/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9029\n",
            "Epoch 14: val_accuracy improved from 0.84447 to 0.85020, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2861 - accuracy: 0.9029 - val_loss: 0.6100 - val_accuracy: 0.8502\n",
            "Epoch 15/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9032\n",
            "Epoch 15: val_accuracy did not improve from 0.85020\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2821 - accuracy: 0.9032 - val_loss: 0.6945 - val_accuracy: 0.8421\n",
            "Epoch 16/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.9043\n",
            "Epoch 16: val_accuracy improved from 0.85020 to 0.85245, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2790 - accuracy: 0.9043 - val_loss: 0.6153 - val_accuracy: 0.8525\n",
            "Epoch 17/100\n",
            "2808/2813 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.9054\n",
            "Epoch 17: val_accuracy did not improve from 0.85245\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2764 - accuracy: 0.9054 - val_loss: 0.8201 - val_accuracy: 0.8320\n",
            "Epoch 18/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.9055\n",
            "Epoch 18: val_accuracy did not improve from 0.85245\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2758 - accuracy: 0.9055 - val_loss: 0.7710 - val_accuracy: 0.8421\n",
            "Epoch 19/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.9060\n",
            "Epoch 19: val_accuracy did not improve from 0.85245\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2734 - accuracy: 0.9060 - val_loss: 0.9782 - val_accuracy: 0.8138\n",
            "Epoch 20/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.9071\n",
            "Epoch 20: val_accuracy improved from 0.85245 to 0.85385, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2705 - accuracy: 0.9071 - val_loss: 0.6825 - val_accuracy: 0.8539\n",
            "Epoch 21/100\n",
            "2808/2813 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.9079\n",
            "Epoch 21: val_accuracy did not improve from 0.85385\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2689 - accuracy: 0.9079 - val_loss: 0.8182 - val_accuracy: 0.8429\n",
            "Epoch 22/100\n",
            "2808/2813 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.9092\n",
            "Epoch 22: val_accuracy did not improve from 0.85385\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2665 - accuracy: 0.9092 - val_loss: 0.8754 - val_accuracy: 0.8289\n",
            "Epoch 23/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9082\n",
            "Epoch 23: val_accuracy did not improve from 0.85385\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2671 - accuracy: 0.9081 - val_loss: 0.9130 - val_accuracy: 0.8268\n",
            "Epoch 24/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9092\n",
            "Epoch 24: val_accuracy improved from 0.85385 to 0.85908, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2639 - accuracy: 0.9092 - val_loss: 0.6303 - val_accuracy: 0.8591\n",
            "Epoch 25/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9089\n",
            "Epoch 25: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2639 - accuracy: 0.9089 - val_loss: 0.9598 - val_accuracy: 0.8228\n",
            "Epoch 26/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2616 - accuracy: 0.9097\n",
            "Epoch 26: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2616 - accuracy: 0.9097 - val_loss: 0.7288 - val_accuracy: 0.8411\n",
            "Epoch 27/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9104\n",
            "Epoch 27: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2612 - accuracy: 0.9104 - val_loss: 0.7455 - val_accuracy: 0.8397\n",
            "Epoch 28/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.9104\n",
            "Epoch 28: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2619 - accuracy: 0.9104 - val_loss: 0.6403 - val_accuracy: 0.8521\n",
            "Epoch 29/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9106\n",
            "Epoch 29: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2597 - accuracy: 0.9106 - val_loss: 0.7611 - val_accuracy: 0.8478\n",
            "Epoch 30/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9107\n",
            "Epoch 30: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2590 - accuracy: 0.9107 - val_loss: 0.8000 - val_accuracy: 0.8407\n",
            "Epoch 31/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9119\n",
            "Epoch 31: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2575 - accuracy: 0.9119 - val_loss: 0.9604 - val_accuracy: 0.8375\n",
            "Epoch 32/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9121\n",
            "Epoch 32: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2565 - accuracy: 0.9121 - val_loss: 0.9434 - val_accuracy: 0.8285\n",
            "Epoch 33/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.9124\n",
            "Epoch 33: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2568 - accuracy: 0.9124 - val_loss: 0.7630 - val_accuracy: 0.8488\n",
            "Epoch 34/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9129\n",
            "Epoch 34: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2548 - accuracy: 0.9128 - val_loss: 0.9045 - val_accuracy: 0.8285\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b500ae510>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformed ResNet18"
      ],
      "metadata": {
        "id": "x74qZyTtJPA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Activation, Dense, Flatten, Conv2D, MaxPooling2D, \n",
        "    GlobalAveragePooling2D, AveragePooling2D, BatchNormalization, add)\n",
        "import tensorflow.keras.regularizers as regulizers"
      ],
      "metadata": {
        "id": "Iui6ZvQMyIvf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _res_conv(filters, kernel_size=3, padding='same', strides=1, use_relu=True, use_bias=False, kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        res = BatchNormalization(axis=-1)(x)\n",
        "        if use_relu:\n",
        "            res = Activation(\"relu\")(res)\n",
        "        conv = Conv2D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=padding, strides=strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(res)\n",
        "        return conv\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "SRQiwtJWyitM"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _merge_with_shortcut(kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x, x_residual):\n",
        "        # We check if `x_residual` was scaled down. If so, we scale `x` accordingly with a 1x1 conv:\n",
        "        x_shape = tf.keras.backend.int_shape(x)\n",
        "        x_residual_shape = tf.keras.backend.int_shape(x_residual)\n",
        "        if x_shape == x_residual_shape:\n",
        "            shortcut = x\n",
        "        else:\n",
        "            strides = (\n",
        "                int(round(x_shape[1] / x_residual_shape[1])), # vertical stride\n",
        "                int(round(x_shape[2] / x_residual_shape[2]))  # horizontal stride\n",
        "            )\n",
        "            x_residual_channels = x_residual_shape[3]\n",
        "            shortcut = Conv2D(\n",
        "                filters=x_residual_channels, kernel_size=(1, 1), padding=\"valid\", strides=strides,\n",
        "                kernel_initializer=kernel_initializer)(x)\n",
        "\n",
        "        merge = add([shortcut, x_residual])\n",
        "        return merge\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "uizHEGVWywHo"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _residual_block_basic(filters, kernel_size=3, strides=1, use_bias=False,\n",
        "                          kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        x_conv1 = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=strides, \n",
        "            use_relu=True, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(x)\n",
        "        dropout = layers.Dropout(0.5)(x_conv1)\n",
        "        x_residual = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=1, \n",
        "            use_relu=False, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(dropout)\n",
        "        merge = _merge_with_shortcut(kernel_initializer)(x, x_residual)\n",
        "        merge = Activation('relu')(merge)\n",
        "        return merge\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "BHVK8hE7xz5L"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _residual_macroblock(block_fn, filters, repetitions=3, kernel_size=3, strides_1st_block=1, use_bias=False,\n",
        "                         kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        for i in range(repetitions):\n",
        "            strides = strides_1st_block if i == 0 else 1\n",
        "            x = block_fn(filters=filters, kernel_size=kernel_size, \n",
        "                         strides=strides, use_bias=use_bias,\n",
        "                         kernel_initializer=kernel_initializer)(x)\n",
        "        return x\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "kjjoCebUyonx"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18(input_shape, num_classes, block_fn=_residual_block_basic, repetitions=(2, 2, 2, 2),\n",
        "           use_bias=True, kernel_initializer='he_normal'):\n",
        "\n",
        "    # Input and 1st layers:\n",
        "    inputs = Input(shape=input_shape)\n",
        "    conv = _res_conv(\n",
        "        filters=64, kernel_size=7, strides=2, use_relu=True, use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer)(inputs)\n",
        "    maxpool = MaxPooling2D(pool_size=3, strides=2, padding='same')(conv)\n",
        "\n",
        "    # Chain of residual blocks:\n",
        "    filters = 64\n",
        "    strides = 2\n",
        "    res_block = maxpool\n",
        "    for i, repet in enumerate(repetitions):\n",
        "        # We do not further reduce the input size for the 1st block (max-pool applied just before):\n",
        "        block_strides = strides if i != 0 else 1\n",
        "        res_block = _residual_macroblock(\n",
        "            block_fn=block_fn, repetitions=repet, filters=filters, strides_1st_block=block_strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(res_block)\n",
        "        filters = min(filters * 2, 1024) # we limit to 1024 filters max\n",
        "\n",
        "    # Final layers for prediction\n",
        "    res_spatial_dim = tf.keras.backend.int_shape(res_block)[1:3]\n",
        "    avg_pool = AveragePooling2D(pool_size=res_spatial_dim, strides=1)(res_block)\n",
        "    glo_avg_pool = GlobalAveragePooling2D()(avg_pool)\n",
        "    predictions = Dense(units=num_classes, kernel_initializer=kernel_initializer, activation='softmax')(glo_avg_pool)\n",
        "\n",
        "    # Model:\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "    return model"
      ],
      "metadata": {
        "id": "P9-KwIK3xl63"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = ResNet18(input_shape=input_shape, num_classes=30)\n",
        "resnet18.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ww75K-0xl1I",
        "outputId": "970e4ef0-bec9-46ed-acf7-50f0853a97f1"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_56 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization_247 (Batch  (None, 28, 28, 1)   4           ['input_56[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_250 (Activation)    (None, 28, 28, 1)    0           ['batch_normalization_247[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_510 (Conv2D)            (None, 14, 14, 64)   3200        ['activation_250[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling2d_82 (MaxPooling2D  (None, 7, 7, 64)    0           ['conv2d_510[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_248 (Batch  (None, 7, 7, 64)    256         ['max_pooling2d_82[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_251 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_248[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_511 (Conv2D)            (None, 7, 7, 64)     36928       ['activation_251[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_142 (Dropout)          (None, 7, 7, 64)     0           ['conv2d_511[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_249 (Batch  (None, 7, 7, 64)    256         ['dropout_142[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_512 (Conv2D)            (None, 7, 7, 64)     36928       ['batch_normalization_249[0][0]']\n",
            "                                                                                                  \n",
            " add_166 (Add)                  (None, 7, 7, 64)     0           ['max_pooling2d_82[0][0]',       \n",
            "                                                                  'conv2d_512[0][0]']             \n",
            "                                                                                                  \n",
            " activation_252 (Activation)    (None, 7, 7, 64)     0           ['add_166[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_250 (Batch  (None, 7, 7, 64)    256         ['activation_252[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_253 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_250[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_513 (Conv2D)            (None, 7, 7, 64)     36928       ['activation_253[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_143 (Dropout)          (None, 7, 7, 64)     0           ['conv2d_513[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_251 (Batch  (None, 7, 7, 64)    256         ['dropout_143[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_514 (Conv2D)            (None, 7, 7, 64)     36928       ['batch_normalization_251[0][0]']\n",
            "                                                                                                  \n",
            " add_167 (Add)                  (None, 7, 7, 64)     0           ['activation_252[0][0]',         \n",
            "                                                                  'conv2d_514[0][0]']             \n",
            "                                                                                                  \n",
            " activation_254 (Activation)    (None, 7, 7, 64)     0           ['add_167[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_252 (Batch  (None, 7, 7, 64)    256         ['activation_254[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_255 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_252[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_515 (Conv2D)            (None, 4, 4, 128)    73856       ['activation_255[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_144 (Dropout)          (None, 4, 4, 128)    0           ['conv2d_515[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_253 (Batch  (None, 4, 4, 128)   512         ['dropout_144[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_517 (Conv2D)            (None, 4, 4, 128)    8320        ['activation_254[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_516 (Conv2D)            (None, 4, 4, 128)    147584      ['batch_normalization_253[0][0]']\n",
            "                                                                                                  \n",
            " add_168 (Add)                  (None, 4, 4, 128)    0           ['conv2d_517[0][0]',             \n",
            "                                                                  'conv2d_516[0][0]']             \n",
            "                                                                                                  \n",
            " activation_256 (Activation)    (None, 4, 4, 128)    0           ['add_168[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_254 (Batch  (None, 4, 4, 128)   512         ['activation_256[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_257 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_254[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_518 (Conv2D)            (None, 4, 4, 128)    147584      ['activation_257[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_145 (Dropout)          (None, 4, 4, 128)    0           ['conv2d_518[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_255 (Batch  (None, 4, 4, 128)   512         ['dropout_145[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_519 (Conv2D)            (None, 4, 4, 128)    147584      ['batch_normalization_255[0][0]']\n",
            "                                                                                                  \n",
            " add_169 (Add)                  (None, 4, 4, 128)    0           ['activation_256[0][0]',         \n",
            "                                                                  'conv2d_519[0][0]']             \n",
            "                                                                                                  \n",
            " activation_258 (Activation)    (None, 4, 4, 128)    0           ['add_169[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_256 (Batch  (None, 4, 4, 128)   512         ['activation_258[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_259 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_256[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_520 (Conv2D)            (None, 2, 2, 256)    295168      ['activation_259[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_146 (Dropout)          (None, 2, 2, 256)    0           ['conv2d_520[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_257 (Batch  (None, 2, 2, 256)   1024        ['dropout_146[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_522 (Conv2D)            (None, 2, 2, 256)    33024       ['activation_258[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_521 (Conv2D)            (None, 2, 2, 256)    590080      ['batch_normalization_257[0][0]']\n",
            "                                                                                                  \n",
            " add_170 (Add)                  (None, 2, 2, 256)    0           ['conv2d_522[0][0]',             \n",
            "                                                                  'conv2d_521[0][0]']             \n",
            "                                                                                                  \n",
            " activation_260 (Activation)    (None, 2, 2, 256)    0           ['add_170[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_258 (Batch  (None, 2, 2, 256)   1024        ['activation_260[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_261 (Activation)    (None, 2, 2, 256)    0           ['batch_normalization_258[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_523 (Conv2D)            (None, 2, 2, 256)    590080      ['activation_261[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_147 (Dropout)          (None, 2, 2, 256)    0           ['conv2d_523[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_259 (Batch  (None, 2, 2, 256)   1024        ['dropout_147[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_524 (Conv2D)            (None, 2, 2, 256)    590080      ['batch_normalization_259[0][0]']\n",
            "                                                                                                  \n",
            " add_171 (Add)                  (None, 2, 2, 256)    0           ['activation_260[0][0]',         \n",
            "                                                                  'conv2d_524[0][0]']             \n",
            "                                                                                                  \n",
            " activation_262 (Activation)    (None, 2, 2, 256)    0           ['add_171[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_260 (Batch  (None, 2, 2, 256)   1024        ['activation_262[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_263 (Activation)    (None, 2, 2, 256)    0           ['batch_normalization_260[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_525 (Conv2D)            (None, 1, 1, 512)    1180160     ['activation_263[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_148 (Dropout)          (None, 1, 1, 512)    0           ['conv2d_525[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_261 (Batch  (None, 1, 1, 512)   2048        ['dropout_148[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_527 (Conv2D)            (None, 1, 1, 512)    131584      ['activation_262[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_526 (Conv2D)            (None, 1, 1, 512)    2359808     ['batch_normalization_261[0][0]']\n",
            "                                                                                                  \n",
            " add_172 (Add)                  (None, 1, 1, 512)    0           ['conv2d_527[0][0]',             \n",
            "                                                                  'conv2d_526[0][0]']             \n",
            "                                                                                                  \n",
            " activation_264 (Activation)    (None, 1, 1, 512)    0           ['add_172[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_262 (Batch  (None, 1, 1, 512)   2048        ['activation_264[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_265 (Activation)    (None, 1, 1, 512)    0           ['batch_normalization_262[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_528 (Conv2D)            (None, 1, 1, 512)    2359808     ['activation_265[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_149 (Dropout)          (None, 1, 1, 512)    0           ['conv2d_528[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_263 (Batch  (None, 1, 1, 512)   2048        ['dropout_149[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_529 (Conv2D)            (None, 1, 1, 512)    2359808     ['batch_normalization_263[0][0]']\n",
            "                                                                                                  \n",
            " add_173 (Add)                  (None, 1, 1, 512)    0           ['activation_264[0][0]',         \n",
            "                                                                  'conv2d_529[0][0]']             \n",
            "                                                                                                  \n",
            " activation_266 (Activation)    (None, 1, 1, 512)    0           ['add_173[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_11 (AverageP  (None, 1, 1, 512)   0           ['activation_266[0][0]']         \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " global_average_pooling2d_15 (G  (None, 512)         0           ['average_pooling2d_11[0][0]']   \n",
            " lobalAveragePooling2D)                                                                           \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 30)           15390       ['global_average_pooling2d_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,194,402\n",
            "Trainable params: 11,187,616\n",
            "Non-trainable params: 6,786\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ugeDhQ6Ry8p-"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, mode = 'auto')\n",
        "\n",
        "#모델 훈련\n",
        "history = resnet18.fit(x_train_after, y_train, batch_size = 64, epochs = 100, shuffle=True, callbacks=[cp_callback, early_stopping], validation_data=(x_test_after, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrSuayHavmME",
        "outputId": "340f7c31-c0cf-4042-d359-a71412bc573b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.4762 - accuracy: 0.8499\n",
            "Epoch 1: val_accuracy improved from -inf to 0.85417, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 78s 27ms/step - loss: 0.4762 - accuracy: 0.8499 - val_loss: 0.6186 - val_accuracy: 0.8542\n",
            "Epoch 2/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9260\n",
            "Epoch 2: val_accuracy did not improve from 0.85417\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.2214 - accuracy: 0.9260 - val_loss: 0.6365 - val_accuracy: 0.8537\n",
            "Epoch 3/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9373\n",
            "Epoch 3: val_accuracy improved from 0.85417 to 0.87345, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1878 - accuracy: 0.9373 - val_loss: 0.5322 - val_accuracy: 0.8734\n",
            "Epoch 4/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.9444\n",
            "Epoch 4: val_accuracy did not improve from 0.87345\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1662 - accuracy: 0.9444 - val_loss: 0.7743 - val_accuracy: 0.8669\n",
            "Epoch 5/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.9488\n",
            "Epoch 5: val_accuracy improved from 0.87345 to 0.87483, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1507 - accuracy: 0.9488 - val_loss: 0.6977 - val_accuracy: 0.8748\n",
            "Epoch 6/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9520\n",
            "Epoch 6: val_accuracy improved from 0.87483 to 0.87617, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1419 - accuracy: 0.9520 - val_loss: 0.6431 - val_accuracy: 0.8762\n",
            "Epoch 7/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9550\n",
            "Epoch 7: val_accuracy improved from 0.87617 to 0.88042, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.1324 - accuracy: 0.9549 - val_loss: 0.5901 - val_accuracy: 0.8804\n",
            "Epoch 8/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9570\n",
            "Epoch 8: val_accuracy improved from 0.88042 to 0.88530, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1251 - accuracy: 0.9570 - val_loss: 0.6476 - val_accuracy: 0.8853\n",
            "Epoch 9/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9591\n",
            "Epoch 9: val_accuracy did not improve from 0.88530\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.1204 - accuracy: 0.9591 - val_loss: 0.7889 - val_accuracy: 0.8801\n",
            "Epoch 10/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9610\n",
            "Epoch 10: val_accuracy improved from 0.88530 to 0.89347, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1154 - accuracy: 0.9610 - val_loss: 0.6644 - val_accuracy: 0.8935\n",
            "Epoch 11/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9620\n",
            "Epoch 11: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 76s 27ms/step - loss: 0.1126 - accuracy: 0.9620 - val_loss: 0.9237 - val_accuracy: 0.8676\n",
            "Epoch 12/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9627\n",
            "Epoch 12: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 77s 27ms/step - loss: 0.1092 - accuracy: 0.9627 - val_loss: 1.1143 - val_accuracy: 0.8723\n",
            "Epoch 13/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9645\n",
            "Epoch 13: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1035 - accuracy: 0.9645 - val_loss: 0.8568 - val_accuracy: 0.8789\n",
            "Epoch 14/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9651\n",
            "Epoch 14: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 76s 27ms/step - loss: 0.1024 - accuracy: 0.9651 - val_loss: 0.8689 - val_accuracy: 0.8829\n",
            "Epoch 15/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9660\n",
            "Epoch 15: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 76s 27ms/step - loss: 0.0998 - accuracy: 0.9660 - val_loss: 0.6470 - val_accuracy: 0.8928\n",
            "Epoch 16/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9668\n",
            "Epoch 16: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0967 - accuracy: 0.9668 - val_loss: 0.9987 - val_accuracy: 0.8796\n",
            "Epoch 17/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9677\n",
            "Epoch 17: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0928 - accuracy: 0.9677 - val_loss: 1.2394 - val_accuracy: 0.8713\n",
            "Epoch 18/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9677\n",
            "Epoch 18: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0933 - accuracy: 0.9677 - val_loss: 0.9746 - val_accuracy: 0.8917\n",
            "Epoch 19/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9686\n",
            "Epoch 19: val_accuracy improved from 0.89347 to 0.89417, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0905 - accuracy: 0.9686 - val_loss: 1.0374 - val_accuracy: 0.8942\n",
            "Epoch 20/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9696\n",
            "Epoch 20: val_accuracy did not improve from 0.89417\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0882 - accuracy: 0.9696 - val_loss: 1.0090 - val_accuracy: 0.8884\n",
            "Epoch 21/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9705\n",
            "Epoch 21: val_accuracy improved from 0.89417 to 0.89732, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 75s 27ms/step - loss: 0.0866 - accuracy: 0.9705 - val_loss: 1.0934 - val_accuracy: 0.8973\n",
            "Epoch 22/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9700\n",
            "Epoch 22: val_accuracy improved from 0.89732 to 0.89777, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0856 - accuracy: 0.9700 - val_loss: 0.8083 - val_accuracy: 0.8978\n",
            "Epoch 23/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9710\n",
            "Epoch 23: val_accuracy improved from 0.89777 to 0.90573, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 0.9986 - val_accuracy: 0.9057\n",
            "Epoch 24/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9724\n",
            "Epoch 24: val_accuracy did not improve from 0.90573\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0799 - accuracy: 0.9724 - val_loss: 0.8123 - val_accuracy: 0.8974\n",
            "Epoch 25/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9715\n",
            "Epoch 25: val_accuracy improved from 0.90573 to 0.90712, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0824 - accuracy: 0.9715 - val_loss: 0.8129 - val_accuracy: 0.9071\n",
            "Epoch 26/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9731\n",
            "Epoch 26: val_accuracy did not improve from 0.90712\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0787 - accuracy: 0.9731 - val_loss: 1.0494 - val_accuracy: 0.8981\n",
            "Epoch 27/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9732\n",
            "Epoch 27: val_accuracy improved from 0.90712 to 0.91553, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 75s 27ms/step - loss: 0.0780 - accuracy: 0.9732 - val_loss: 0.6717 - val_accuracy: 0.9155\n",
            "Epoch 28/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9737\n",
            "Epoch 28: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0750 - accuracy: 0.9737 - val_loss: 1.0869 - val_accuracy: 0.8884\n",
            "Epoch 29/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9736\n",
            "Epoch 29: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0781 - accuracy: 0.9735 - val_loss: 1.1049 - val_accuracy: 0.9029\n",
            "Epoch 30/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9748\n",
            "Epoch 30: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0719 - accuracy: 0.9748 - val_loss: 1.1707 - val_accuracy: 0.8913\n",
            "Epoch 31/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9743\n",
            "Epoch 31: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0769 - accuracy: 0.9743 - val_loss: 0.7410 - val_accuracy: 0.9043\n",
            "Epoch 32/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9749\n",
            "Epoch 32: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0720 - accuracy: 0.9749 - val_loss: 0.9794 - val_accuracy: 0.8937\n",
            "Epoch 33/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9755\n",
            "Epoch 33: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0712 - accuracy: 0.9755 - val_loss: 1.2311 - val_accuracy: 0.8895\n",
            "Epoch 34/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9757\n",
            "Epoch 34: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0701 - accuracy: 0.9757 - val_loss: 1.0953 - val_accuracy: 0.9032\n",
            "Epoch 35/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9760\n",
            "Epoch 35: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0704 - accuracy: 0.9760 - val_loss: 1.9884 - val_accuracy: 0.8716\n",
            "Epoch 36/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9762\n",
            "Epoch 36: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0700 - accuracy: 0.9762 - val_loss: 1.2774 - val_accuracy: 0.8969\n",
            "Epoch 37/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9770\n",
            "Epoch 37: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0664 - accuracy: 0.9770 - val_loss: 1.6283 - val_accuracy: 0.8846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augumentation + Transformed ResNet18 Ver 2."
      ],
      "metadata": {
        "id": "_tATtJe-Q0pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _res_conv(filters, kernel_size=3, padding='same', strides=1, use_relu=True, use_bias=False, kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        res = BatchNormalization(axis=-1)(x)\n",
        "        if use_relu:\n",
        "            res = Activation(\"relu\")(res)\n",
        "        conv = Conv2D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=padding, strides=strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(res)\n",
        "        return conv\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "5rWUUGSoIjbb"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _merge_with_shortcut(kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x, x_residual):\n",
        "        # We check if `x_residual` was scaled down. If so, we scale `x` accordingly with a 1x1 conv:\n",
        "        x_shape = tf.keras.backend.int_shape(x)\n",
        "        x_residual_shape = tf.keras.backend.int_shape(x_residual)\n",
        "        if x_shape == x_residual_shape:\n",
        "            shortcut = x\n",
        "        else:\n",
        "            strides = (\n",
        "                int(round(x_shape[1] / x_residual_shape[1])), # vertical stride\n",
        "                int(round(x_shape[2] / x_residual_shape[2]))  # horizontal stride\n",
        "            )\n",
        "            x_residual_channels = x_residual_shape[3]\n",
        "            shortcut = Conv2D(\n",
        "                filters=x_residual_channels, kernel_size=(1, 1), padding=\"valid\", strides=strides,\n",
        "                kernel_initializer=kernel_initializer)(x)\n",
        "\n",
        "        merge = add([shortcut, x_residual])\n",
        "        return merge\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "AzCN3o4VIkMS"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _residual_block_basic(filters, kernel_size=3, strides=1, use_bias=False,\n",
        "                          kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        x_conv1 = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=strides, \n",
        "            use_relu=True, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(x)\n",
        "        dropout = layers.Dropout(0.5)(x_conv1)\n",
        "        x_residual = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=1, \n",
        "            use_relu=False, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(dropout)\n",
        "        merge = _merge_with_shortcut(kernel_initializer)(x, x_residual)\n",
        "        merge = Activation('relu')(merge)\n",
        "        return merge\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "5sYMQt-CIoZ7"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _residual_macroblock(block_fn, filters, repetitions=3, kernel_size=3, strides_1st_block=1, use_bias=False,\n",
        "                         kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        for i in range(repetitions):\n",
        "            strides = strides_1st_block if i == 0 else 1\n",
        "            x = block_fn(filters=filters, kernel_size=kernel_size, \n",
        "                         strides=strides, use_bias=use_bias,\n",
        "                         kernel_initializer=kernel_initializer)(x)\n",
        "        return x\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "28ve7XBGItB0"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18(input_shape, num_classes, block_fn=_residual_block_basic, repetitions=(2, 2, 2, 2),\n",
        "           use_bias=True, kernel_initializer='he_normal'):\n",
        "\n",
        "    # Input and 1st layers:\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    conv = _res_conv(\n",
        "        filters=64, kernel_size=7, strides=2, use_relu=True, use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer)(x)\n",
        "    avgpool = AveragePooling2D(pool_size=3, strides=2, padding='same')(conv)\n",
        "\n",
        "    # Chain of residual blocks:\n",
        "    filters = 64\n",
        "    strides = 2\n",
        "    res_block = avgpool\n",
        "    for i, repet in enumerate(repetitions):\n",
        "        # We do not further reduce the input size for the 1st block (max-pool applied just before):\n",
        "        block_strides = strides if i != 0 else 1\n",
        "        res_block = _residual_macroblock(\n",
        "            block_fn=block_fn, repetitions=repet, filters=filters, strides_1st_block=block_strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(res_block)\n",
        "        filters = min(filters * 2, 1024) # we limit to 1024 filters max\n",
        "\n",
        "    # Final layers for prediction\n",
        "    res_spatial_dim = tf.keras.backend.int_shape(res_block)[1:3]\n",
        "    avg_pool = AveragePooling2D(pool_size=res_spatial_dim, strides=1)(res_block)\n",
        "    glo_avg_pool = GlobalAveragePooling2D()(avg_pool)\n",
        "    predictions = Dense(units=num_classes, kernel_initializer=kernel_initializer, activation='softmax')(glo_avg_pool)\n",
        "\n",
        "    # Model:\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "    return model"
      ],
      "metadata": {
        "id": "qbVo8JYVI3Lw"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = ResNet18(input_shape=input_shape, num_classes=30)\n",
        "resnet18.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh7o2b9zJBa9",
        "outputId": "12c7e8d5-de70-4991-f14f-59253c28d2fd"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 200, 200, 1)  0           ['input_59[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_298 (Batch  (None, 200, 200, 1)  4          ['sequential_2[0][0]']           \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_301 (Activation)    (None, 200, 200, 1)  0           ['batch_normalization_298[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_570 (Conv2D)            (None, 100, 100, 64  3200        ['activation_301[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " average_pooling2d_14 (AverageP  (None, 50, 50, 64)  0           ['conv2d_570[0][0]']             \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " batch_normalization_299 (Batch  (None, 50, 50, 64)  256         ['average_pooling2d_14[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_302 (Activation)    (None, 50, 50, 64)   0           ['batch_normalization_299[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_571 (Conv2D)            (None, 50, 50, 64)   36928       ['activation_302[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_166 (Dropout)          (None, 50, 50, 64)   0           ['conv2d_571[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_300 (Batch  (None, 50, 50, 64)  256         ['dropout_166[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_572 (Conv2D)            (None, 50, 50, 64)   36928       ['batch_normalization_300[0][0]']\n",
            "                                                                                                  \n",
            " add_190 (Add)                  (None, 50, 50, 64)   0           ['average_pooling2d_14[0][0]',   \n",
            "                                                                  'conv2d_572[0][0]']             \n",
            "                                                                                                  \n",
            " activation_303 (Activation)    (None, 50, 50, 64)   0           ['add_190[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_301 (Batch  (None, 50, 50, 64)  256         ['activation_303[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_304 (Activation)    (None, 50, 50, 64)   0           ['batch_normalization_301[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_573 (Conv2D)            (None, 50, 50, 64)   36928       ['activation_304[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_167 (Dropout)          (None, 50, 50, 64)   0           ['conv2d_573[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_302 (Batch  (None, 50, 50, 64)  256         ['dropout_167[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_574 (Conv2D)            (None, 50, 50, 64)   36928       ['batch_normalization_302[0][0]']\n",
            "                                                                                                  \n",
            " add_191 (Add)                  (None, 50, 50, 64)   0           ['activation_303[0][0]',         \n",
            "                                                                  'conv2d_574[0][0]']             \n",
            "                                                                                                  \n",
            " activation_305 (Activation)    (None, 50, 50, 64)   0           ['add_191[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_303 (Batch  (None, 50, 50, 64)  256         ['activation_305[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_306 (Activation)    (None, 50, 50, 64)   0           ['batch_normalization_303[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_575 (Conv2D)            (None, 25, 25, 128)  73856       ['activation_306[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_168 (Dropout)          (None, 25, 25, 128)  0           ['conv2d_575[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_304 (Batch  (None, 25, 25, 128)  512        ['dropout_168[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_577 (Conv2D)            (None, 25, 25, 128)  8320        ['activation_305[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_576 (Conv2D)            (None, 25, 25, 128)  147584      ['batch_normalization_304[0][0]']\n",
            "                                                                                                  \n",
            " add_192 (Add)                  (None, 25, 25, 128)  0           ['conv2d_577[0][0]',             \n",
            "                                                                  'conv2d_576[0][0]']             \n",
            "                                                                                                  \n",
            " activation_307 (Activation)    (None, 25, 25, 128)  0           ['add_192[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_305 (Batch  (None, 25, 25, 128)  512        ['activation_307[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_308 (Activation)    (None, 25, 25, 128)  0           ['batch_normalization_305[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_578 (Conv2D)            (None, 25, 25, 128)  147584      ['activation_308[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_169 (Dropout)          (None, 25, 25, 128)  0           ['conv2d_578[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_306 (Batch  (None, 25, 25, 128)  512        ['dropout_169[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_579 (Conv2D)            (None, 25, 25, 128)  147584      ['batch_normalization_306[0][0]']\n",
            "                                                                                                  \n",
            " add_193 (Add)                  (None, 25, 25, 128)  0           ['activation_307[0][0]',         \n",
            "                                                                  'conv2d_579[0][0]']             \n",
            "                                                                                                  \n",
            " activation_309 (Activation)    (None, 25, 25, 128)  0           ['add_193[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_307 (Batch  (None, 25, 25, 128)  512        ['activation_309[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_310 (Activation)    (None, 25, 25, 128)  0           ['batch_normalization_307[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_580 (Conv2D)            (None, 13, 13, 256)  295168      ['activation_310[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_170 (Dropout)          (None, 13, 13, 256)  0           ['conv2d_580[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_308 (Batch  (None, 13, 13, 256)  1024       ['dropout_170[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_582 (Conv2D)            (None, 13, 13, 256)  33024       ['activation_309[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_581 (Conv2D)            (None, 13, 13, 256)  590080      ['batch_normalization_308[0][0]']\n",
            "                                                                                                  \n",
            " add_194 (Add)                  (None, 13, 13, 256)  0           ['conv2d_582[0][0]',             \n",
            "                                                                  'conv2d_581[0][0]']             \n",
            "                                                                                                  \n",
            " activation_311 (Activation)    (None, 13, 13, 256)  0           ['add_194[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_309 (Batch  (None, 13, 13, 256)  1024       ['activation_311[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_312 (Activation)    (None, 13, 13, 256)  0           ['batch_normalization_309[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_583 (Conv2D)            (None, 13, 13, 256)  590080      ['activation_312[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_171 (Dropout)          (None, 13, 13, 256)  0           ['conv2d_583[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_310 (Batch  (None, 13, 13, 256)  1024       ['dropout_171[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_584 (Conv2D)            (None, 13, 13, 256)  590080      ['batch_normalization_310[0][0]']\n",
            "                                                                                                  \n",
            " add_195 (Add)                  (None, 13, 13, 256)  0           ['activation_311[0][0]',         \n",
            "                                                                  'conv2d_584[0][0]']             \n",
            "                                                                                                  \n",
            " activation_313 (Activation)    (None, 13, 13, 256)  0           ['add_195[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_311 (Batch  (None, 13, 13, 256)  1024       ['activation_313[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_314 (Activation)    (None, 13, 13, 256)  0           ['batch_normalization_311[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_585 (Conv2D)            (None, 7, 7, 512)    1180160     ['activation_314[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_172 (Dropout)          (None, 7, 7, 512)    0           ['conv2d_585[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_312 (Batch  (None, 7, 7, 512)   2048        ['dropout_172[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_587 (Conv2D)            (None, 7, 7, 512)    131584      ['activation_313[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_586 (Conv2D)            (None, 7, 7, 512)    2359808     ['batch_normalization_312[0][0]']\n",
            "                                                                                                  \n",
            " add_196 (Add)                  (None, 7, 7, 512)    0           ['conv2d_587[0][0]',             \n",
            "                                                                  'conv2d_586[0][0]']             \n",
            "                                                                                                  \n",
            " activation_315 (Activation)    (None, 7, 7, 512)    0           ['add_196[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_313 (Batch  (None, 7, 7, 512)   2048        ['activation_315[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_316 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_313[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_588 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_316[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_173 (Dropout)          (None, 7, 7, 512)    0           ['conv2d_588[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_314 (Batch  (None, 7, 7, 512)   2048        ['dropout_173[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_589 (Conv2D)            (None, 7, 7, 512)    2359808     ['batch_normalization_314[0][0]']\n",
            "                                                                                                  \n",
            " add_197 (Add)                  (None, 7, 7, 512)    0           ['activation_315[0][0]',         \n",
            "                                                                  'conv2d_589[0][0]']             \n",
            "                                                                                                  \n",
            " activation_317 (Activation)    (None, 7, 7, 512)    0           ['add_197[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_15 (AverageP  (None, 1, 1, 512)   0           ['activation_317[0][0]']         \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " global_average_pooling2d_18 (G  (None, 512)         0           ['average_pooling2d_15[0][0]']   \n",
            " lobalAveragePooling2D)                                                                           \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 30)           15390       ['global_average_pooling2d_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,194,402\n",
            "Trainable params: 11,187,616\n",
            "Non-trainable params: 6,786\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ly1UAPUhQrP1"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, mode = 'auto')\n",
        "\n",
        "#모델 훈련\n",
        "history = resnet18.fit(x_train_after, y_train, batch_size = 64, epochs = 100, shuffle=True, callbacks=[cp_callback, early_stopping], validation_data=(x_test_after, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVu1-SWqJkRL",
        "outputId": "9bf29a0e-7d1d-4a61-c3c3-a10e2a7f24e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8555\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84320, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 660s 233ms/step - loss: 0.4469 - accuracy: 0.8555 - val_loss: 0.5603 - val_accuracy: 0.8432\n",
            "Epoch 2/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9349\n",
            "Epoch 2: val_accuracy did not improve from 0.84320\n",
            "2813/2813 [==============================] - 645s 229ms/step - loss: 0.1849 - accuracy: 0.9349 - val_loss: 0.6863 - val_accuracy: 0.8424\n",
            "Epoch 3/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9462\n",
            "Epoch 3: val_accuracy did not improve from 0.84320\n",
            "2813/2813 [==============================] - 643s 229ms/step - loss: 0.1524 - accuracy: 0.9462 - val_loss: 0.7751 - val_accuracy: 0.7861\n",
            "Epoch 4/100\n",
            "1714/2813 [=================>............] - ETA: 3:55 - loss: 0.1338 - accuracy: 0.9522"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#결과 그래프 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "#단순한 특성 추출 방식의 훈련 정확도와 검증 정확도 그래프 그리기\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "#단순한 특성 추출 방식의 훈련 손실과 검증 손실 그래프 그리기\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Z1zzdjHcjCKt",
        "outputId": "89f1c9b2-fbce-46cd-9bd8-a78835c1695c"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18"
      ],
      "metadata": {
        "id": "yHZtkFh-3ZIg"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team04'\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_'+ team_name + '.h5')"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aPbgI-c-Kj8"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WONVxH-Kt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc0334b-0c40-41f4-bf0c-c69391780c6a"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team04'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + team_name + '.h5')\n",
        "\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.6283 - accuracy: 0.8846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6282691955566406, 0.8846250176429749]"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    }
  ]
}