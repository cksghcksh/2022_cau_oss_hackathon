{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1AosAX9DXOlc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cksghcksh/2022_cau_oss_hackathon/blob/main/hackathon_team04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        "*   모든 구현은 [2. 데이터 전처리] 및 [3.모델 생성]에서만 진행\n",
        "*   [4. 모델 저장]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *    트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임 -> 런타임 연결 해제 및 삭제\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        " *    \"런타임 -> 런타임 다시시작\"은 클라우드 스토리지에 저장된 모델은 유지됨\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        " *  모델 발표 자료 \n",
        "* 제출 기한: **오후 6시 (단, 발표자료는 12시)**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2022_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드가 적발될 경우\n",
        "*  Pre-trained 모델을 사용한 경우 (transfer learning 포함)\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4525d49c-aec2-4850-e453-5befe478efb5"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from keras import datasets, layers, models\n",
        "\n",
        "# 데이터셋 로드 (MNIST, fashion-MNIST, Kujushiji-MNIST, MNIST_corrupted (test only))\n",
        "train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "\n",
        "train_ds2, test_ds2 = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds2['label'] += 10;\n",
        "test_ds2['label'] += 10;\n",
        "\n",
        "train_ds3, test_ds3 = tfds.load('kmnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds3['label'] += 20;\n",
        "test_ds3['label'] += 20;\n",
        "\n",
        "test_ds4 = tfds.load('mnist_corrupted/zigzag', split='test', shuffle_files=False, batch_size=-1)\n",
        "\n",
        "# 데이터셋 병합 (training: 180,000개, test: 40,000개)\n",
        "x_train = np.append(np.append(train_ds['image'], train_ds2['image'], 0), train_ds3['image'], 0);\n",
        "y_train = np.append(np.append(train_ds['label'], train_ds2['label'], 0), train_ds3['label'], 0);\n",
        "\n",
        "x_test = np.append(np.append(np.append(test_ds['image'], test_ds2['image'], 0), test_ds3['image'], 0), test_ds4['image'], 0);\n",
        "y_test = np.append(np.append(np.append(test_ds['label'], test_ds2['label'], 0), test_ds3['label'], 0), test_ds4['label'], 0);\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수: 30, 입력 데이터 구조: (28, 28, 1)\n",
        "num_classes = y_train.shape[1]\n",
        "input_shape = x_train.shape[1:]\n",
        "print(num_classes, input_shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 (28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9"
      },
      "source": [
        "# **2. 데이터 전처리**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9KWTBP6AI1"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "# 원본 데이터와 전처리 후 데이터를 구분하기 위해, 변수명 x_train_after, x_test_after를 변경하지 말 것\n",
        "x_train_after = x_train / 255\n",
        "x_test_after = x_test / 255"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-lo-O1yiFpY"
      },
      "source": [
        "# **3. 모델 생성**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## residual block을 포함한 CNN"
      ],
      "metadata": {
        "id": "cYdqJmRyJH7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = data_augmentation(inputs) #랜덤하게 증식된 이미지 데이터 입력\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "#x = BatchNormalization()(x)\n",
        "#x = layers.Activation(\"leakyrelu\")\n",
        "residual = x\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2, padding=\"same\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "residual = layers.Conv2D(32, 1, strides=2)(residual)\n",
        "x = layers.add([x, residual])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "residual = x\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2, padding=\"same\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)\n",
        "x = layers.add([x, residual])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "outputs = layers.Dense(30, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWm8fHmGiYiO",
        "outputId": "24f5d4b0-9b51-4d75-8b01-5b220ed22fe1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 28, 28, 1)    0           ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)             (None, 26, 26, 32)   320         ['sequential[10][0]']            \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)            (None, 26, 26, 32)   9248        ['conv2d_99[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)            (None, 26, 26, 32)   9248        ['conv2d_100[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_21 (MaxPooling2D  (None, 13, 13, 32)  0           ['conv2d_101[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 13, 13, 32)   0           ['max_pooling2d_21[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)            (None, 13, 13, 32)   1056        ['conv2d_99[0][0]']              \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 13, 13, 32)   0           ['dropout_20[0][0]',             \n",
            "                                                                  'conv2d_102[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 13, 13, 32)   0           ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)            (None, 11, 11, 64)   18496       ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)            (None, 11, 11, 64)   36928       ['conv2d_103[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)            (None, 11, 11, 64)   36928       ['conv2d_104[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_22 (MaxPooling2D  (None, 6, 6, 64)    0           ['conv2d_105[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 6, 6, 64)     0           ['max_pooling2d_22[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)            (None, 6, 6, 64)     4160        ['conv2d_103[0][0]']             \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 6, 6, 64)     0           ['dropout_22[0][0]',             \n",
            "                                                                  'conv2d_106[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 6, 6, 64)     0           ['add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_107 (Conv2D)            (None, 4, 4, 128)    73856       ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)            (None, 2, 2, 128)    147584      ['conv2d_107[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_23 (MaxPooling2D  (None, 1, 1, 128)   0           ['conv2d_108[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3 (Gl  (None, 128)         0           ['max_pooling2d_23[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 30)           3870        ['global_average_pooling2d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 341,694\n",
            "Trainable params: 341,694\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, mode = 'auto')\n",
        "\n",
        "#모델 훈련\n",
        "model.fit(x_train_after, y_train, batch_size = 64, epochs = 100, shuffle=True, callbacks=[cp_callback, early_stopping], validation_data=(x_test_after, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvutNnjXibab",
        "outputId": "3c8fb3bf-e586-45aa-c757-c679790373ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.8844 - accuracy: 0.7131\n",
            "Epoch 1: val_accuracy improved from -inf to 0.73410, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 33s 11ms/step - loss: 0.8843 - accuracy: 0.7132 - val_loss: 1.0925 - val_accuracy: 0.7341\n",
            "Epoch 2/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.4900 - accuracy: 0.8340\n",
            "Epoch 2: val_accuracy improved from 0.73410 to 0.78882, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 32s 11ms/step - loss: 0.4899 - accuracy: 0.8340 - val_loss: 0.8035 - val_accuracy: 0.7888\n",
            "Epoch 3/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.8597\n",
            "Epoch 3: val_accuracy improved from 0.78882 to 0.79137, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.4123 - accuracy: 0.8597 - val_loss: 0.8657 - val_accuracy: 0.7914\n",
            "Epoch 4/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8705\n",
            "Epoch 4: val_accuracy improved from 0.79137 to 0.81767, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.3770 - accuracy: 0.8705 - val_loss: 0.6848 - val_accuracy: 0.8177\n",
            "Epoch 5/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8793\n",
            "Epoch 5: val_accuracy improved from 0.81767 to 0.83368, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.3502 - accuracy: 0.8793 - val_loss: 0.7003 - val_accuracy: 0.8337\n",
            "Epoch 6/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.8850\n",
            "Epoch 6: val_accuracy improved from 0.83368 to 0.83947, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 32s 11ms/step - loss: 0.3361 - accuracy: 0.8850 - val_loss: 0.5880 - val_accuracy: 0.8395\n",
            "Epoch 7/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8893\n",
            "Epoch 7: val_accuracy did not improve from 0.83947\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.3232 - accuracy: 0.8893 - val_loss: 0.7167 - val_accuracy: 0.8266\n",
            "Epoch 8/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8929\n",
            "Epoch 8: val_accuracy improved from 0.83947 to 0.84447, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.3126 - accuracy: 0.8929 - val_loss: 0.6270 - val_accuracy: 0.8445\n",
            "Epoch 9/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8942\n",
            "Epoch 9: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 32s 11ms/step - loss: 0.3082 - accuracy: 0.8942 - val_loss: 0.6411 - val_accuracy: 0.8352\n",
            "Epoch 10/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8971\n",
            "Epoch 10: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 33s 12ms/step - loss: 0.2986 - accuracy: 0.8971 - val_loss: 0.6719 - val_accuracy: 0.8423\n",
            "Epoch 11/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.8988\n",
            "Epoch 11: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 33s 12ms/step - loss: 0.2955 - accuracy: 0.8988 - val_loss: 0.7580 - val_accuracy: 0.8339\n",
            "Epoch 12/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8993\n",
            "Epoch 12: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2922 - accuracy: 0.8993 - val_loss: 0.7899 - val_accuracy: 0.8337\n",
            "Epoch 13/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9012\n",
            "Epoch 13: val_accuracy did not improve from 0.84447\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2876 - accuracy: 0.9012 - val_loss: 0.6640 - val_accuracy: 0.8431\n",
            "Epoch 14/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9029\n",
            "Epoch 14: val_accuracy improved from 0.84447 to 0.85020, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2861 - accuracy: 0.9029 - val_loss: 0.6100 - val_accuracy: 0.8502\n",
            "Epoch 15/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9032\n",
            "Epoch 15: val_accuracy did not improve from 0.85020\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2821 - accuracy: 0.9032 - val_loss: 0.6945 - val_accuracy: 0.8421\n",
            "Epoch 16/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.9043\n",
            "Epoch 16: val_accuracy improved from 0.85020 to 0.85245, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2790 - accuracy: 0.9043 - val_loss: 0.6153 - val_accuracy: 0.8525\n",
            "Epoch 17/100\n",
            "2808/2813 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.9054\n",
            "Epoch 17: val_accuracy did not improve from 0.85245\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2764 - accuracy: 0.9054 - val_loss: 0.8201 - val_accuracy: 0.8320\n",
            "Epoch 18/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.9055\n",
            "Epoch 18: val_accuracy did not improve from 0.85245\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2758 - accuracy: 0.9055 - val_loss: 0.7710 - val_accuracy: 0.8421\n",
            "Epoch 19/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.9060\n",
            "Epoch 19: val_accuracy did not improve from 0.85245\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2734 - accuracy: 0.9060 - val_loss: 0.9782 - val_accuracy: 0.8138\n",
            "Epoch 20/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.9071\n",
            "Epoch 20: val_accuracy improved from 0.85245 to 0.85385, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2705 - accuracy: 0.9071 - val_loss: 0.6825 - val_accuracy: 0.8539\n",
            "Epoch 21/100\n",
            "2808/2813 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.9079\n",
            "Epoch 21: val_accuracy did not improve from 0.85385\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2689 - accuracy: 0.9079 - val_loss: 0.8182 - val_accuracy: 0.8429\n",
            "Epoch 22/100\n",
            "2808/2813 [============================>.] - ETA: 0s - loss: 0.2666 - accuracy: 0.9092\n",
            "Epoch 22: val_accuracy did not improve from 0.85385\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2665 - accuracy: 0.9092 - val_loss: 0.8754 - val_accuracy: 0.8289\n",
            "Epoch 23/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9082\n",
            "Epoch 23: val_accuracy did not improve from 0.85385\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2671 - accuracy: 0.9081 - val_loss: 0.9130 - val_accuracy: 0.8268\n",
            "Epoch 24/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9092\n",
            "Epoch 24: val_accuracy improved from 0.85385 to 0.85908, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2639 - accuracy: 0.9092 - val_loss: 0.6303 - val_accuracy: 0.8591\n",
            "Epoch 25/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9089\n",
            "Epoch 25: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2639 - accuracy: 0.9089 - val_loss: 0.9598 - val_accuracy: 0.8228\n",
            "Epoch 26/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2616 - accuracy: 0.9097\n",
            "Epoch 26: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2616 - accuracy: 0.9097 - val_loss: 0.7288 - val_accuracy: 0.8411\n",
            "Epoch 27/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9104\n",
            "Epoch 27: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2612 - accuracy: 0.9104 - val_loss: 0.7455 - val_accuracy: 0.8397\n",
            "Epoch 28/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.9104\n",
            "Epoch 28: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2619 - accuracy: 0.9104 - val_loss: 0.6403 - val_accuracy: 0.8521\n",
            "Epoch 29/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9106\n",
            "Epoch 29: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2597 - accuracy: 0.9106 - val_loss: 0.7611 - val_accuracy: 0.8478\n",
            "Epoch 30/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9107\n",
            "Epoch 30: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2590 - accuracy: 0.9107 - val_loss: 0.8000 - val_accuracy: 0.8407\n",
            "Epoch 31/100\n",
            "2809/2813 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9119\n",
            "Epoch 31: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2575 - accuracy: 0.9119 - val_loss: 0.9604 - val_accuracy: 0.8375\n",
            "Epoch 32/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9121\n",
            "Epoch 32: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2565 - accuracy: 0.9121 - val_loss: 0.9434 - val_accuracy: 0.8285\n",
            "Epoch 33/100\n",
            "2810/2813 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.9124\n",
            "Epoch 33: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 31s 11ms/step - loss: 0.2568 - accuracy: 0.9124 - val_loss: 0.7630 - val_accuracy: 0.8488\n",
            "Epoch 34/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9129\n",
            "Epoch 34: val_accuracy did not improve from 0.85908\n",
            "2813/2813 [==============================] - 30s 11ms/step - loss: 0.2548 - accuracy: 0.9128 - val_loss: 0.9045 - val_accuracy: 0.8285\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b500ae510>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformed ResNet18"
      ],
      "metadata": {
        "id": "x74qZyTtJPA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Activation, Dense, Flatten, Conv2D, MaxPooling2D, \n",
        "    GlobalAveragePooling2D, AveragePooling2D, BatchNormalization, add)\n",
        "import tensorflow.keras.regularizers as regulizers"
      ],
      "metadata": {
        "id": "Iui6ZvQMyIvf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _res_conv(filters, kernel_size=3, padding='same', strides=1, use_relu=True, use_bias=False, kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        res = BatchNormalization(axis=-1)(x)\n",
        "        if use_relu:\n",
        "            res = Activation(\"relu\")(res)\n",
        "        conv = Conv2D(\n",
        "            filters=filters, kernel_size=kernel_size, padding=padding, strides=strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(res)\n",
        "        return conv\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "SRQiwtJWyitM"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _merge_with_shortcut(kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x, x_residual):\n",
        "        # We check if `x_residual` was scaled down. If so, we scale `x` accordingly with a 1x1 conv:\n",
        "        x_shape = tf.keras.backend.int_shape(x)\n",
        "        x_residual_shape = tf.keras.backend.int_shape(x_residual)\n",
        "        if x_shape == x_residual_shape:\n",
        "            shortcut = x\n",
        "        else:\n",
        "            strides = (\n",
        "                int(round(x_shape[1] / x_residual_shape[1])), # vertical stride\n",
        "                int(round(x_shape[2] / x_residual_shape[2]))  # horizontal stride\n",
        "            )\n",
        "            x_residual_channels = x_residual_shape[3]\n",
        "            shortcut = Conv2D(\n",
        "                filters=x_residual_channels, kernel_size=(1, 1), padding=\"valid\", strides=strides,\n",
        "                kernel_initializer=kernel_initializer)(x)\n",
        "\n",
        "        merge = add([shortcut, x_residual])\n",
        "        return merge\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "uizHEGVWywHo"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _residual_block_basic(filters, kernel_size=3, strides=1, use_bias=False,\n",
        "                          kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        x_conv1 = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=strides, \n",
        "            use_relu=True, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(x)\n",
        "        dropout = layers.Dropout(0.5)(x_conv1)\n",
        "        x_residual = _res_conv(\n",
        "            filters=filters, kernel_size=kernel_size, padding='same', strides=1, \n",
        "            use_relu=False, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(dropout)\n",
        "        merge = _merge_with_shortcut(kernel_initializer)(x, x_residual)\n",
        "        merge = Activation('relu')(merge)\n",
        "        return merge\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "BHVK8hE7xz5L"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _residual_macroblock(block_fn, filters, repetitions=3, kernel_size=3, strides_1st_block=1, use_bias=False,\n",
        "                         kernel_initializer='he_normal'):\n",
        "\n",
        "    def layer_fn(x):\n",
        "        for i in range(repetitions):\n",
        "            strides = strides_1st_block if i == 0 else 1\n",
        "            x = block_fn(filters=filters, kernel_size=kernel_size, \n",
        "                         strides=strides, use_bias=use_bias,\n",
        "                         kernel_initializer=kernel_initializer)(x)\n",
        "        return x\n",
        "\n",
        "    return layer_fn"
      ],
      "metadata": {
        "id": "kjjoCebUyonx"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18(input_shape, num_classes, block_fn=_residual_block_basic, repetitions=(2, 2, 2, 2),\n",
        "           use_bias=True, kernel_initializer='he_normal'):\n",
        "\n",
        "    # Input and 1st layers:\n",
        "    inputs = Input(shape=input_shape)\n",
        "    conv = _res_conv(\n",
        "        filters=64, kernel_size=7, strides=2, use_relu=True, use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer)(inputs)\n",
        "    maxpool = MaxPooling2D(pool_size=3, strides=2, padding='same')(conv)\n",
        "\n",
        "    # Chain of residual blocks:\n",
        "    filters = 64\n",
        "    strides = 2\n",
        "    res_block = maxpool\n",
        "    for i, repet in enumerate(repetitions):\n",
        "        # We do not further reduce the input size for the 1st block (max-pool applied just before):\n",
        "        block_strides = strides if i != 0 else 1\n",
        "        res_block = _residual_macroblock(\n",
        "            block_fn=block_fn, repetitions=repet, filters=filters, strides_1st_block=block_strides, use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer)(res_block)\n",
        "        filters = min(filters * 2, 1024) # we limit to 1024 filters max\n",
        "\n",
        "    # Final layers for prediction\n",
        "    res_spatial_dim = tf.keras.backend.int_shape(res_block)[1:3]\n",
        "    avg_pool = AveragePooling2D(pool_size=res_spatial_dim, strides=1)(res_block)\n",
        "    glo_avg_pool = GlobalAveragePooling2D()(avg_pool)\n",
        "    predictions = Dense(units=num_classes, kernel_initializer=kernel_initializer, activation='softmax')(glo_avg_pool)\n",
        "\n",
        "    # Model:\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "    return model"
      ],
      "metadata": {
        "id": "P9-KwIK3xl63"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = ResNet18(input_shape=input_shape, num_classes=30)\n",
        "resnet18.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ww75K-0xl1I",
        "outputId": "970e4ef0-bec9-46ed-acf7-50f0853a97f1"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_56 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization_247 (Batch  (None, 28, 28, 1)   4           ['input_56[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_250 (Activation)    (None, 28, 28, 1)    0           ['batch_normalization_247[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_510 (Conv2D)            (None, 14, 14, 64)   3200        ['activation_250[0][0]']         \n",
            "                                                                                                  \n",
            " max_pooling2d_82 (MaxPooling2D  (None, 7, 7, 64)    0           ['conv2d_510[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_248 (Batch  (None, 7, 7, 64)    256         ['max_pooling2d_82[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_251 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_248[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_511 (Conv2D)            (None, 7, 7, 64)     36928       ['activation_251[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_142 (Dropout)          (None, 7, 7, 64)     0           ['conv2d_511[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_249 (Batch  (None, 7, 7, 64)    256         ['dropout_142[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_512 (Conv2D)            (None, 7, 7, 64)     36928       ['batch_normalization_249[0][0]']\n",
            "                                                                                                  \n",
            " add_166 (Add)                  (None, 7, 7, 64)     0           ['max_pooling2d_82[0][0]',       \n",
            "                                                                  'conv2d_512[0][0]']             \n",
            "                                                                                                  \n",
            " activation_252 (Activation)    (None, 7, 7, 64)     0           ['add_166[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_250 (Batch  (None, 7, 7, 64)    256         ['activation_252[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_253 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_250[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_513 (Conv2D)            (None, 7, 7, 64)     36928       ['activation_253[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_143 (Dropout)          (None, 7, 7, 64)     0           ['conv2d_513[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_251 (Batch  (None, 7, 7, 64)    256         ['dropout_143[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_514 (Conv2D)            (None, 7, 7, 64)     36928       ['batch_normalization_251[0][0]']\n",
            "                                                                                                  \n",
            " add_167 (Add)                  (None, 7, 7, 64)     0           ['activation_252[0][0]',         \n",
            "                                                                  'conv2d_514[0][0]']             \n",
            "                                                                                                  \n",
            " activation_254 (Activation)    (None, 7, 7, 64)     0           ['add_167[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_252 (Batch  (None, 7, 7, 64)    256         ['activation_254[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_255 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_252[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_515 (Conv2D)            (None, 4, 4, 128)    73856       ['activation_255[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_144 (Dropout)          (None, 4, 4, 128)    0           ['conv2d_515[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_253 (Batch  (None, 4, 4, 128)   512         ['dropout_144[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_517 (Conv2D)            (None, 4, 4, 128)    8320        ['activation_254[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_516 (Conv2D)            (None, 4, 4, 128)    147584      ['batch_normalization_253[0][0]']\n",
            "                                                                                                  \n",
            " add_168 (Add)                  (None, 4, 4, 128)    0           ['conv2d_517[0][0]',             \n",
            "                                                                  'conv2d_516[0][0]']             \n",
            "                                                                                                  \n",
            " activation_256 (Activation)    (None, 4, 4, 128)    0           ['add_168[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_254 (Batch  (None, 4, 4, 128)   512         ['activation_256[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_257 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_254[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_518 (Conv2D)            (None, 4, 4, 128)    147584      ['activation_257[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_145 (Dropout)          (None, 4, 4, 128)    0           ['conv2d_518[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_255 (Batch  (None, 4, 4, 128)   512         ['dropout_145[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_519 (Conv2D)            (None, 4, 4, 128)    147584      ['batch_normalization_255[0][0]']\n",
            "                                                                                                  \n",
            " add_169 (Add)                  (None, 4, 4, 128)    0           ['activation_256[0][0]',         \n",
            "                                                                  'conv2d_519[0][0]']             \n",
            "                                                                                                  \n",
            " activation_258 (Activation)    (None, 4, 4, 128)    0           ['add_169[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_256 (Batch  (None, 4, 4, 128)   512         ['activation_258[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_259 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_256[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_520 (Conv2D)            (None, 2, 2, 256)    295168      ['activation_259[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_146 (Dropout)          (None, 2, 2, 256)    0           ['conv2d_520[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_257 (Batch  (None, 2, 2, 256)   1024        ['dropout_146[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_522 (Conv2D)            (None, 2, 2, 256)    33024       ['activation_258[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_521 (Conv2D)            (None, 2, 2, 256)    590080      ['batch_normalization_257[0][0]']\n",
            "                                                                                                  \n",
            " add_170 (Add)                  (None, 2, 2, 256)    0           ['conv2d_522[0][0]',             \n",
            "                                                                  'conv2d_521[0][0]']             \n",
            "                                                                                                  \n",
            " activation_260 (Activation)    (None, 2, 2, 256)    0           ['add_170[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_258 (Batch  (None, 2, 2, 256)   1024        ['activation_260[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_261 (Activation)    (None, 2, 2, 256)    0           ['batch_normalization_258[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_523 (Conv2D)            (None, 2, 2, 256)    590080      ['activation_261[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_147 (Dropout)          (None, 2, 2, 256)    0           ['conv2d_523[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_259 (Batch  (None, 2, 2, 256)   1024        ['dropout_147[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_524 (Conv2D)            (None, 2, 2, 256)    590080      ['batch_normalization_259[0][0]']\n",
            "                                                                                                  \n",
            " add_171 (Add)                  (None, 2, 2, 256)    0           ['activation_260[0][0]',         \n",
            "                                                                  'conv2d_524[0][0]']             \n",
            "                                                                                                  \n",
            " activation_262 (Activation)    (None, 2, 2, 256)    0           ['add_171[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_260 (Batch  (None, 2, 2, 256)   1024        ['activation_262[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_263 (Activation)    (None, 2, 2, 256)    0           ['batch_normalization_260[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_525 (Conv2D)            (None, 1, 1, 512)    1180160     ['activation_263[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_148 (Dropout)          (None, 1, 1, 512)    0           ['conv2d_525[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_261 (Batch  (None, 1, 1, 512)   2048        ['dropout_148[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_527 (Conv2D)            (None, 1, 1, 512)    131584      ['activation_262[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_526 (Conv2D)            (None, 1, 1, 512)    2359808     ['batch_normalization_261[0][0]']\n",
            "                                                                                                  \n",
            " add_172 (Add)                  (None, 1, 1, 512)    0           ['conv2d_527[0][0]',             \n",
            "                                                                  'conv2d_526[0][0]']             \n",
            "                                                                                                  \n",
            " activation_264 (Activation)    (None, 1, 1, 512)    0           ['add_172[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_262 (Batch  (None, 1, 1, 512)   2048        ['activation_264[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_265 (Activation)    (None, 1, 1, 512)    0           ['batch_normalization_262[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_528 (Conv2D)            (None, 1, 1, 512)    2359808     ['activation_265[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_149 (Dropout)          (None, 1, 1, 512)    0           ['conv2d_528[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_263 (Batch  (None, 1, 1, 512)   2048        ['dropout_149[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_529 (Conv2D)            (None, 1, 1, 512)    2359808     ['batch_normalization_263[0][0]']\n",
            "                                                                                                  \n",
            " add_173 (Add)                  (None, 1, 1, 512)    0           ['activation_264[0][0]',         \n",
            "                                                                  'conv2d_529[0][0]']             \n",
            "                                                                                                  \n",
            " activation_266 (Activation)    (None, 1, 1, 512)    0           ['add_173[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_11 (AverageP  (None, 1, 1, 512)   0           ['activation_266[0][0]']         \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " global_average_pooling2d_15 (G  (None, 512)         0           ['average_pooling2d_11[0][0]']   \n",
            " lobalAveragePooling2D)                                                                           \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 30)           15390       ['global_average_pooling2d_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,194,402\n",
            "Trainable params: 11,187,616\n",
            "Non-trainable params: 6,786\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ugeDhQ6Ry8p-"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, mode = 'auto')\n",
        "\n",
        "#모델 훈련\n",
        "history = resnet18.fit(x_train_after, y_train, batch_size = 64, epochs = 100, shuffle=True, callbacks=[cp_callback, early_stopping], validation_data=(x_test_after, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrSuayHavmME",
        "outputId": "340f7c31-c0cf-4042-d359-a71412bc573b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.4762 - accuracy: 0.8499\n",
            "Epoch 1: val_accuracy improved from -inf to 0.85417, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 78s 27ms/step - loss: 0.4762 - accuracy: 0.8499 - val_loss: 0.6186 - val_accuracy: 0.8542\n",
            "Epoch 2/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9260\n",
            "Epoch 2: val_accuracy did not improve from 0.85417\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.2214 - accuracy: 0.9260 - val_loss: 0.6365 - val_accuracy: 0.8537\n",
            "Epoch 3/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9373\n",
            "Epoch 3: val_accuracy improved from 0.85417 to 0.87345, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1878 - accuracy: 0.9373 - val_loss: 0.5322 - val_accuracy: 0.8734\n",
            "Epoch 4/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.9444\n",
            "Epoch 4: val_accuracy did not improve from 0.87345\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1662 - accuracy: 0.9444 - val_loss: 0.7743 - val_accuracy: 0.8669\n",
            "Epoch 5/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.9488\n",
            "Epoch 5: val_accuracy improved from 0.87345 to 0.87483, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1507 - accuracy: 0.9488 - val_loss: 0.6977 - val_accuracy: 0.8748\n",
            "Epoch 6/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9520\n",
            "Epoch 6: val_accuracy improved from 0.87483 to 0.87617, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1419 - accuracy: 0.9520 - val_loss: 0.6431 - val_accuracy: 0.8762\n",
            "Epoch 7/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9550\n",
            "Epoch 7: val_accuracy improved from 0.87617 to 0.88042, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.1324 - accuracy: 0.9549 - val_loss: 0.5901 - val_accuracy: 0.8804\n",
            "Epoch 8/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9570\n",
            "Epoch 8: val_accuracy improved from 0.88042 to 0.88530, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1251 - accuracy: 0.9570 - val_loss: 0.6476 - val_accuracy: 0.8853\n",
            "Epoch 9/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9591\n",
            "Epoch 9: val_accuracy did not improve from 0.88530\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.1204 - accuracy: 0.9591 - val_loss: 0.7889 - val_accuracy: 0.8801\n",
            "Epoch 10/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9610\n",
            "Epoch 10: val_accuracy improved from 0.88530 to 0.89347, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1154 - accuracy: 0.9610 - val_loss: 0.6644 - val_accuracy: 0.8935\n",
            "Epoch 11/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9620\n",
            "Epoch 11: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 76s 27ms/step - loss: 0.1126 - accuracy: 0.9620 - val_loss: 0.9237 - val_accuracy: 0.8676\n",
            "Epoch 12/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9627\n",
            "Epoch 12: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 77s 27ms/step - loss: 0.1092 - accuracy: 0.9627 - val_loss: 1.1143 - val_accuracy: 0.8723\n",
            "Epoch 13/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9645\n",
            "Epoch 13: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.1035 - accuracy: 0.9645 - val_loss: 0.8568 - val_accuracy: 0.8789\n",
            "Epoch 14/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9651\n",
            "Epoch 14: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 76s 27ms/step - loss: 0.1024 - accuracy: 0.9651 - val_loss: 0.8689 - val_accuracy: 0.8829\n",
            "Epoch 15/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9660\n",
            "Epoch 15: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 76s 27ms/step - loss: 0.0998 - accuracy: 0.9660 - val_loss: 0.6470 - val_accuracy: 0.8928\n",
            "Epoch 16/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9668\n",
            "Epoch 16: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0967 - accuracy: 0.9668 - val_loss: 0.9987 - val_accuracy: 0.8796\n",
            "Epoch 17/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9677\n",
            "Epoch 17: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0928 - accuracy: 0.9677 - val_loss: 1.2394 - val_accuracy: 0.8713\n",
            "Epoch 18/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9677\n",
            "Epoch 18: val_accuracy did not improve from 0.89347\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0933 - accuracy: 0.9677 - val_loss: 0.9746 - val_accuracy: 0.8917\n",
            "Epoch 19/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9686\n",
            "Epoch 19: val_accuracy improved from 0.89347 to 0.89417, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0905 - accuracy: 0.9686 - val_loss: 1.0374 - val_accuracy: 0.8942\n",
            "Epoch 20/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9696\n",
            "Epoch 20: val_accuracy did not improve from 0.89417\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0882 - accuracy: 0.9696 - val_loss: 1.0090 - val_accuracy: 0.8884\n",
            "Epoch 21/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9705\n",
            "Epoch 21: val_accuracy improved from 0.89417 to 0.89732, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 75s 27ms/step - loss: 0.0866 - accuracy: 0.9705 - val_loss: 1.0934 - val_accuracy: 0.8973\n",
            "Epoch 22/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9700\n",
            "Epoch 22: val_accuracy improved from 0.89732 to 0.89777, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0856 - accuracy: 0.9700 - val_loss: 0.8083 - val_accuracy: 0.8978\n",
            "Epoch 23/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9710\n",
            "Epoch 23: val_accuracy improved from 0.89777 to 0.90573, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 0.9986 - val_accuracy: 0.9057\n",
            "Epoch 24/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9724\n",
            "Epoch 24: val_accuracy did not improve from 0.90573\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0799 - accuracy: 0.9724 - val_loss: 0.8123 - val_accuracy: 0.8974\n",
            "Epoch 25/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9715\n",
            "Epoch 25: val_accuracy improved from 0.90573 to 0.90712, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0824 - accuracy: 0.9715 - val_loss: 0.8129 - val_accuracy: 0.9071\n",
            "Epoch 26/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9731\n",
            "Epoch 26: val_accuracy did not improve from 0.90712\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0787 - accuracy: 0.9731 - val_loss: 1.0494 - val_accuracy: 0.8981\n",
            "Epoch 27/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9732\n",
            "Epoch 27: val_accuracy improved from 0.90712 to 0.91553, saving model to /content/checkpoint_entire_best.h5\n",
            "2813/2813 [==============================] - 75s 27ms/step - loss: 0.0780 - accuracy: 0.9732 - val_loss: 0.6717 - val_accuracy: 0.9155\n",
            "Epoch 28/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9737\n",
            "Epoch 28: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0750 - accuracy: 0.9737 - val_loss: 1.0869 - val_accuracy: 0.8884\n",
            "Epoch 29/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9736\n",
            "Epoch 29: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0781 - accuracy: 0.9735 - val_loss: 1.1049 - val_accuracy: 0.9029\n",
            "Epoch 30/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9748\n",
            "Epoch 30: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0719 - accuracy: 0.9748 - val_loss: 1.1707 - val_accuracy: 0.8913\n",
            "Epoch 31/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9743\n",
            "Epoch 31: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0769 - accuracy: 0.9743 - val_loss: 0.7410 - val_accuracy: 0.9043\n",
            "Epoch 32/100\n",
            "2811/2813 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9749\n",
            "Epoch 32: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0720 - accuracy: 0.9749 - val_loss: 0.9794 - val_accuracy: 0.8937\n",
            "Epoch 33/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9755\n",
            "Epoch 33: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0712 - accuracy: 0.9755 - val_loss: 1.2311 - val_accuracy: 0.8895\n",
            "Epoch 34/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9757\n",
            "Epoch 34: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 73s 26ms/step - loss: 0.0701 - accuracy: 0.9757 - val_loss: 1.0953 - val_accuracy: 0.9032\n",
            "Epoch 35/100\n",
            "2813/2813 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9760\n",
            "Epoch 35: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0704 - accuracy: 0.9760 - val_loss: 1.9884 - val_accuracy: 0.8716\n",
            "Epoch 36/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9762\n",
            "Epoch 36: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0700 - accuracy: 0.9762 - val_loss: 1.2774 - val_accuracy: 0.8969\n",
            "Epoch 37/100\n",
            "2812/2813 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9770\n",
            "Epoch 37: val_accuracy did not improve from 0.91553\n",
            "2813/2813 [==============================] - 74s 26ms/step - loss: 0.0664 - accuracy: 0.9770 - val_loss: 1.6283 - val_accuracy: 0.8846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18"
      ],
      "metadata": {
        "id": "yHZtkFh-3ZIg"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team04'\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_'+ team_name + '.h5')"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aPbgI-c-Kj8"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WONVxH-Kt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc0334b-0c40-41f4-bf0c-c69391780c6a"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team04'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + team_name + '.h5')\n",
        "\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 9s 7ms/step - loss: 1.6283 - accuracy: 0.8846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6282691955566406, 0.8846250176429749]"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    }
  ]
}